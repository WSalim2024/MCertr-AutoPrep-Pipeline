# AutoPrep-Pipeline

**A modular, automated data preprocessing tool with an interactive Dashboard.**

## üìå Overview
**AutoPrep-Pipeline** is a comprehensive solution designed to bridge the gap between raw, messy data and clean, model-ready datasets. It features both a **Python automation script** for backend processing and a **Streamlit Web UI** for interactive testing and visualization.

This project demonstrates core competencies in:
- **Data Engineering:** Automated ingestion and handling of missing values.
- **Full-Stack Data Science:** Building user interfaces for data workflows.
- **Statistical Analysis:** Z-score outlier detection and distribution scaling.
- **Machine Learning Prep:** One-Hot Encoding for categorical feature transformation.

## ‚ú® Features
- **üìä Interactive Dashboard:** A user-friendly web interface to upload, clean, and download data instantly.
- **‚ö° Automated Ingestion:** Loads CSV datasets seamlessly using Pandas.
- **üßπ Smart Cleaning:** Detects and handles missing values (imputation or removal).
- **üìâ Outlier Detection:** Implements Z-score analysis to identify and remove statistical outliers.
- **üìè Normalization:** Supports Min-Max scaling to standardise features.
- **üî¢ Encoding:** Converts categorical variables using One-Hot Encoding.

## üìÇ Project Structure
```text
AutoPrep-Pipeline/
‚îÇ
‚îú‚îÄ‚îÄ app.py                    # FRONTEND: Streamlit interactive dashboard
‚îú‚îÄ‚îÄ data_cleaner.py           # CORE LOGIC: The main class-based preprocessing pipeline
‚îú‚îÄ‚îÄ generate_dummy_data.py    # UTILITY: Generates synthetic 'messy' data for testing
‚îú‚îÄ‚îÄ requirements.txt          # DEPENDENCIES: List of required Python libraries
‚îú‚îÄ‚îÄ raw_data.csv              # ARTIFACT: The input file (generated by the utility)
‚îî‚îÄ‚îÄ README.md                 # DOCUMENTATION: Project overview and instructions
```
## ‚ú® Features
- **Automated Ingestion:** Loads CSV datasets seamlessly using Pandas.
- **Smart Cleaning:** Detects and handles missing values (imputation or removal).
- **Outlier Detection:** Implements Z-score analysis to identify and remove statistical outliers.
- **Normalization:** Supports Min-Max scaling and Z-score standardization.
- **Encoding:** Converts categorical variables using One-Hot Encoding.

## üìã Prerequisites
Ensure you have Python 3.x installed. The project relies on the following libraries:
- `pandas` (Data Manipulation)
- `numpy` (Numerical Operations)
- `scikit-learn` (Preprocessing)
- `scipy` (Statistical Scoring)

## ‚öôÔ∏è Installation
1. Clone the repository:
```bash
  pip install pandas numpy scikit-learn missingno matplotlib
```
2. Install dependencies:
```bash
  pip install -r requirements.txt
```
## üöÄ Usage Guide

#### _**Option 1:**_ The Interactive Dashboard (Recommended)

Launch the local web server to use the visual interface:
```bash
streamlit run app.py
```
* Opens automatically in your browser.
* Click "Generate Dummy Data" to create a test set.
* Click "Run Preprocessing Pipeline" to clean it.
* Download the result as a CSV.

#### **_Option 2: The Command Line Script**_

If you prefer running the backend logic directly:
Generate Test Data:

```bash
python generate_dummy_data.py
```
Run the Pipeline:
```bash
python data_cleaner.py
```

# üß† Technical Workflow
The DataPreprocessor class executes the following sequential steps:

1. Ingestion: Loads raw_data.csv into a Pandas DataFrame.
2. Imputation: Identifies NaN values in numeric columns and fills them with the column mean (Strategy: fill_mean).
3. Outlier Removal: Calculates the Z-score for all numeric entries. Rows containing values > 3 standard deviations from the mean are dropped.
4. Scaling: Applies Min-Max Scaling to normalize continuous variables to a [0, 1] range.
5. Encoding: Converts categorical variables into binary vectors using One-Hot Encoding.
6. Export: Saves the sanitized dataset.

## ‚úçÔ∏è Author
* **Waqar Salim**

## ‚ö†Ô∏è Disclaimer
This project is intended for educational purposes as part of a Masters Data Science portfolio. It is designed to simulate data cleaning on synthetic data and should be tested thoroughly before use in production environments.
